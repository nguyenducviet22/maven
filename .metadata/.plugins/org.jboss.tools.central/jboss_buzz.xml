<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>DataCater uses Quarkus to make Data Streaming more accessible</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/datacater-uses-quarkus-to-make-data-streaming-accessible/&#xA;            " /><author><name>Stefan Sprenger (https://twitter.com/flipping_bits)</name></author><id>https://quarkus.io/blog/datacater-uses-quarkus-to-make-data-streaming-accessible/</id><updated>2023-02-09T00:00:00Z</updated><published>2023-02-09T00:00:00Z</published><summary type="html">This article gives a brief overview of the data streaming platform DataCater, discusses how we moved from Scala Play! and Kafka Streams to Quarkus, and presents why we think that Quarkus is an exceptional framework for developing cloud-native Java applications. What is DataCater? DataCater is a real-time, cloud-native data pipeline...</summary><dc:creator>Stefan Sprenger (https://twitter.com/flipping_bits)</dc:creator><dc:date>2023-02-09T00:00:00Z</dc:date></entry><entry><title type="html">Synchronize your KIE Sandbox workspace with Bitbucket or GitHub</title><link rel="alternate" href="https://blog.kie.org/2023/02/synchronize-your-kie-sandbox-workspace-with-bitbucket-or-github.html" /><author><name>Jan Stastny</name></author><id>https://blog.kie.org/2023/02/synchronize-your-kie-sandbox-workspace-with-bitbucket-or-github.html</id><updated>2023-02-08T18:31:37Z</updated><content type="html">1 INTRODUCTION  KIE Sandbox now brings the possibility to synchronize your changes not only with GitHub, but also Bitbucket.  &gt; Bitbucket has several variants of their offering. Only support for Bitbucket &gt; Cloud is available, namely bitbucket.org – a publicly hosted instance. Other &gt; Bitbucket variants, being it Bitbucket Server or Bitbucket Data Center are not &gt; supported, due to significant differences in provided APIs. Support for enterprise instances of Bitbucket Cloud will arrive in future.  2 CONNECTING TO AN ACCOUNT  AUTHORIZATION  For KIE Sandbox to be able to communicate with a given git instance, it needs to be provided a way to authenticate with. In the case of Bitbucket, the supported way is to use a so-called App password, in the case of GitHub, its name is Personal Access Token. They are long-lasting OAuth2 tokens with selected scopes that enable the actions in KIE Sandbox when working with our workspace.  List of related OAuth2 scopes:  -------------------------------------------------------------------------------- Bitbucket scopeDescriptionFor the initial authentication and user details retrieval (e.g., uuid of the user)  Read access to repositories of given user.Write access to all the repositories the authorizing user has access to. To create repositories Read access to all the snippets the authorizing user has access to.  Write access to all the snippets the authorizing user can edit Required Bitbucket OAuth2 scopes () -------------------------------------------------------------------------------- GitHub scopeDescription(no-scope)Implicit read access to public repositories and Gists. repoFull access to public and private repositories.gistGrants write access to Gists.  Required GitHub OAuth2 scopes () -------------------------------------------------------------------------------- KIE SANDBOX CONNECTED ACCOUNTS  Once we have a token, we need to configure the KIE Sandbox to make it available in there.   You can configure the authentication in advance by visiting the Connected Accounts section in the top panel. KIE Sandbox serves as a modal where the user needs to specify required values. In the case of Bitbucket, it is a username and the app password OAuth2 token. This is a difference compared to GitHub, where the token itself carries the user information and is sufficient for authentication on its own.  After the addition, you should see the account information together with usage statistics in your current instance of KIE Sandbox.   Once you connect the account, it’s available in your workspaces through the Authentication source section in the Share dropdown. Where you can also connect to another account by using the respective link from the dropdown, which serves as a shortcut to the Connected Accounts section mentioned earlier.  3 COLLABORATION IS KEY (KIE)  It’s always better to share. And with KIE Sandbox sharing is as easy as a single click to synchronize your changes with the remote location.  &gt; KIE Sandbox does not aspire at replacing the interaction with the existing git &gt; tools or git vendor UI, rather it simplifies most common operations vital for &gt; the smooth experience when modelling your assets.  COLLABORATE OVER AN EXISTING REPOSITORY  The ability to import projects and later synchronize your local changes back into the original location makes KIE Sandbox collaborative experience a breeze.  IMPORT FROM URL  It all starts with a URL. You can import existing projects on KIE Sandbox main screen by pasting their respective URL into the From URL widget. Once it’s confirmed as supported, the widget itself queries git-related information to be used and presents it to the user allowing to override the identified defaults, e.g., to change a branch to be checked out (click Change… if you want to do so).  After the project is imported, you’re taken into its workspace which is readily configured with the respective Authentication source, so you’re ready to Push or Pull without any further action.  SUPPORTED IMPORT URL OPTIONS  HostnameURL Contextbitbucket.org /:workspace/:repo   /:workspace/:repo/src/:tree   /:workspace/:repo/src/:tree/:path  /:workspace/workspace/snippets/:snippet_id/:snippet_name  /:workspace/workspace/snippets/:snippet_id/:snippet_name#file-:path  /snippets/:workspace/:snippet_id/:snippet_name.git github.com /:org/:repo  /:org/:repo/tree/:tree  /:org/:repo/tree/:tree/:path* raw.githubusercontent.com/:org/:repo/:tree/:path* gist.github.com or gist.githubusercontent.com /:user/:gistId/  /:user/:gistId/raw/:fileId/:fileName  /:gistId  SHARING LOCAL PROJECTS WITH THE WORLD  Having created a complex project including several models, now you wonder how to get those to your colleagues? No need to download and send by email, just create a brand-new git repository based on your workspace contents. By picking a different Authentication source, you can select between all your connected git vendor accounts.  PICK AN AUTHENTICATION SOURCE  The first step is to decide which Authentication source we’d like to use to share our workspace. Choice defines the actions allowed with the workspace further on.  Editor toolbar reflects this decision in its Share dropdown, where only actions applicable to your setup are available upon Authentication source selection.    Once you pick any of these options, your project is transformed into git-based workspace. Though there are some differences in how you can synchronize the contents, see them described below.  CREATE A GIT REPOSITORY  Create a repository by clicking on the respective button provided in the Share dropdown. Afterwards a modal appears where you can specify an intended location, repository name, and its visibility.  Location must be specified using a dynamically populated select field. The list of options is possible to be reload using the sync icon to its right.  In the case of GitHub, we can choose between creating the repository under our user location and creating in a GitHub organization that we are members of.   In the case of Bitbucket we select from either personal or shared workspaces. The repository name text field is populated with the value matching your workspace name, so if you’ve configured your workspace thoroughly, you should be all set. Though you are free to specify the name that suits your needs, with just a few limitations on the characters allowed guarded by validation. The Public and Private checkboxes define the repository visibility.  After the repository is created, you’re taken back into workspace, which is readily configured with the respective Authentication source, so you’re ready to Push or Pull without any further action.  CREATE A BITBUCKET SNIPPET OR GITHUB GIST  Do you hesitate to create a separate repository for something you were just doodling? Then share it just as a Bitbucket Snippet or GitHub Gist. Both these options provide you with the ability to push or pull changes, share by URL, etc.  After clicking the respective button, a new Modal dialog is displayed. We must specify  * a location under which the item will be created.  * In the case of GitHub Gist, the only option is the user account itself, it is not supported by GitHub to create a Gist in an organization. The select field is thus displayed as disabled.  * On the other hand, Bitbucket allows us to create Snippets generally in any workspace, as long as we have necessary permissions.  * Visibility of the item being created.  After confirming the dialog, the modal stays displayed until the operation is completed. After that notification alert with the newly created item is displayed. When the creation succeeds, you’re taken back into the workspace, which is now configured accordingly.  In comparison to the regular git repository Sync options above, you now have just a single operation listed there – update.  This action allows you to promote the changes back into the original location.  There is no choice to pull, i.e., update your workspace based on changes done either in Gist or Snippet. If you need a more flexible approach, let’s head for the following section.  TURN GIST OR SNIPPET INTO A REGULAR GIT REPOSITORY  Working with a Snippet or Gist and you’re finally satisfied with the models? Or do you want a more flexible workflow requiring updating your workspace with remote changes?  Turn your project into a regular git repository – the Share dropdown option is still there for Gist or Snippet–based workspaces. You’re not even limited to the same Authentication source here, e.g., for a GitHub Gist based workspace, you can easily share it as a Bitbucket repository afterwards, given that you have both accounts connected.  SUMMARY  * ✅We’ve seen how users can integrate KIE Sandbox into their git-based workflow.  * 💥KIE Sandbox now supports GitHub and Bitbucket Cloud.  * 💥Toolbar buttons now reflect the currently selected Authentication provider.  * 💥GitHub Repositories can also be created in GitHub organizations.  The post appeared first on .</content><dc:creator>Jan Stastny</dc:creator></entry><entry><title type="html">CloudEvents labeling and classification with Drools</title><link rel="alternate" href="https://blog.kie.org/2023/02/cloudevents-labeling-and-classification-with-drools.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2023/02/cloudevents-labeling-and-classification-with-drools.html</id><updated>2023-02-08T15:40:00Z</updated><content type="html">This blog post is a quick update on a demo in labeling CNCF’s CloudEvents. INTRODUCTION Categorizing events is a general, common use-case; in the context of this post, we will delve into labeling CNCF’s CloudEvents for Intelligent Response Management (IRM) which can find application in several ways. One way is to categorize and prioritize different types of events based on their urgency or importance; for example: a SRE team might label an event as "critical" if it involves a major service outage, or "low priority" if it is a minor issue of a sub-system that can be resolved at a later time. This allows the team to quickly respond to the most pressing issues and allocate resources accordingly. Additionally, labeling events can also be used to track and analyze patterns in a system (or cluster) behaviors, which can help to identify potential problems before they occur and improve the overall reliability of the system by implementing corrective actions preventively. This demo make use of several technologies: * and YaRD for the rule definition and evaluation * for cloud native based decisioning * for cloud native Java development on top of Kubernetes * –this is the format of the event data that we want to process * Kafka as an event broker * PostgreSQL as our data store; we’re using specifically PostgreSQL for very interesting query capabilities that this RDBMS can offer * to define our custom types on top of PostgreSQL * extension * for the web-based GUI ARCHITECTURE The following is a high level diagram of the overall architecture for this demo: On the left hand side, the incoming CloudEvents instance that we want to process by labeling, is received by the endpoint which represents one of the possible inputs for this application. The CloudEvents instance is then immediately placed on a kafka topic, which is used to better isolate the ingress portion of this application from the rest of the processing pipeline. The processing pipeline starts with a labeling processor: this is the component responsible for applying the rules to enrich the CloudEvents instance with the required and applicable labels. As a result, the received message is now enriched with labeling and it gets persisted inside the data store. PostgreSQL is used specifically here as it provides hierarchical labels via ltree data type and related query capabilities, which are very useful in categorization applications such as this one. These advanced query capabilities are also foundational to potentially re-process the same CloudEvents instance, after some further augmentation or additional manual labeling. In the context of this article, the web-based GUI is provisional and will be used only as a practical demonstrator for the rich query capabilities. WALKTHROUGH A CloudEvents instance is submitted to this application, for example: { "specversion": "1.0", "id": "matteo-8eb9-43b2-9313-22133f2c747a", "source": "example", "type": "demo20220715contextlabel.demotype", "data": { "host": "basedidati.milano.local", "diskPerc": 70, "memPerc": 50, "cpuPerc": 20 } } The data context of the CloudEvents instance pertains to some host which came under supervision due to resource load. We now want to classify this context/case, using some labels. We may have more than one label. Each label is hierarchical (root.branch1.branch2.leaf). We want to classify the hostname based on its relevance to the department, unit, person or team responsible for it. To do so, a simple decision table provides an easy solution. For example, we can classify the hostname based on geographical location or determine the type of server based on the hostname. Ultimately, we might want to setup a labeling rule for who’s on call, something like the following decision table using : type: DecisionTable inputs: ['.location', '.type'] rules: - when: ['startswith("location.emea")', '. == "type.db"'] then: '"oncall.EMEA.dbadm"' - when: ['startswith("location.emea") | not', '. == "type.db"'] then: '"oncall.CORP.dbadm"' - when: ['true', '. == "type.nas"'] then: '"oncall.CORP.it"' For example, a CloudEvents context may be labeled as follows: * type.db * location.emea.italy.milan * oncall.EMEA.dbadm For the PostgreSQL DDL we currently have: Table "public.cecase" Column | Type | Collation | Nullable | Default ---------+------------------------+-----------+----------+--------- id | bigint | | not null | ceuuid | character varying(255) | | | context | jsonb | | | mytag | ltree[] | | | Indexes: "cecase_pkey" PRIMARY KEY, btree (id) "mytag_gist_idx" gist (mytag) "mytag_idx" btree (mytag) Please notice we’re taking advantage here of PostgreSQL’s jsonb for storing the original CloudEvents context, and ltree[] data type for searching ad-hoc with indexing the hierarchical labels. The latter is extremely helpful also to setup queries making use of and ~ operators for PostgreSQL which performs on the ltree data type, showcased below. As the data flows into the application, we can use the provisional web-based GUI which provide a convenient way to consume the backend REST API(s) developed on Quarkus: In the screenshot above, you can access all the records from the table, where the labels have been applied by the rule definition. We can browse by having at least one label having the specified parent, with a query like: SELECT * FROM cecase WHERE mytag &lt;@ 'oncall.CORP' For example, if we want all the records having at least a label for the oncall.CORP rooting: We can browse by having at least one label having the specified ltree, with a query like” SELECT * FROM cecase WHERE mytag ~ *.emea.* For example, if we want all the records having at least a label for the .emea. (a branch named emea in any point in the hierarchical label): Don’t forget to check out the video linked above, as it demonstrates the demo working live as the data is being sent to the application! If you want to checkout the source, here is the code repo:   These advanced query capabilities offered by PostgreSQL can be used as a foundation to identify events due to reprocessing, manual inspection, triggering a workflow, etc. …but that is maybe subject for a second iteration on this demo… CONCLUSIONS This demo showcases the power of combining declarative logic, persistence and other technologies to process and label CloudEvents effectively! We defined our logic using a combination of expression and rules in the form of decision tables, combined with the use of PostgreSQL as a data store thanks to its advanced query capabilities, allowing for a more efficient and effective handling of the events. We hope you enjoyed our demo and look forward to hearing your feedback! The post appeared first on .</content><dc:creator>Matteo Mortari</dc:creator></entry><entry><title type="html">How does Knative project compare to Dapr?</title><link rel="alternate" href="http://www.ofbizian.com/2023/02/how-does-knative-project-compare-to-dapr.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2023/02/how-does-knative-project-compare-to-dapr.html</id><updated>2023-02-08T10:30:00Z</updated><content type="html">HOW DOES KNATIVE PROJECT COMPARE TO DAPR? Both and projects help create and run cloud-native applications on Kubernetes, but differ in important aspects. I thought I'd quickly share where these projects overlap and complement each other from a user point of view. If you prefer, you can read this post as a twitter too. Dapr vs Knative TLDR: Knative extends Kubernetes with serverless containers (scaling to and from 0) and helps you connect applications declaratively. Dapr helps developers implement reliable connected distributed applications quickly.  COMMUNITY Knative originated from Google, whereas Dapr from Microsoft. Today both projects are incubating at CNCF. Both projects have growing communities and are within top 20 active CNCF projects (Dapr #10 and Knative #17) Community statistics See the full CNCF project statistics . PRIMARY FOCUS AREA ➤Knative extends Kubernetes with serverless containers by taking care of runtine networking (sync/async), autoscaling (to/from zero), and app revision tracking. ➤Dapr helps developers create reliable connected distributed applications quickly. It doesn’t manage the lifecycle of the application, instead it runs next to the applications. TARGET USER ➤Knative serving can be used by Ops to auto-scale and release apps (with traffic splitting). Knative eventing and functions can be used by devs to build, deploy apps, and connect external systems and event-driven containers.  ➤Dapr is toolkit designed primarily for developers. Developers use APIs &amp;amp; SDKs to interact with Dapr and offload responsibilities such as: pub/sub, state access, stateful service invocation, resiliency, etc. There are design time and runtime benefits for architects and operations teams respectively as described . SUPPORTED PLATFORMS ➤Knative runs only on top of Kubernetes and a network layer such as Kourier, Istio, Contour. ➤Dapr can run on Kubernetes, as well as on-premises and edge devices (such as the ). For local dev, Knative requires Kubernetes, whereas Dapr can also run on Docker, or as a single binary only. DEPLOYMENT MODEL Both projects have operator, helm chats, CLI that help with installation and operating the control planes on Kubernetes.  ➤On the data plane side, Dapr is a sidecar that gets injected into the application pod. The application interacts with Dapr over well-defined APIs... ➤Knative dictates how the application is defined and run on Kubernetes by creating deployments, pods, configmaps, and networking configurations. It injects a transparent sidecar into every pod to measure network activity. And has an activator to hold off requests while scaling from zero. DEVELOPER EXPERIENCE ➤Knative uses Kubernetes CRDs for defining an app (called Knative Service) composed of container, configuration, revision. It also offers CRDs that can define how events (CloudEvents) flow between these services, subscribe to a broker. And functions-centric CLI. ➤Dapr offers HTTP and gRPC APIs for Service invocation, Pub/Sub, State, Workflows, Bindings, Configuration, Secrets, Distributed lock, called building blocks. Devs use an HTTP/gRPC client, or SDKs for 8+ languages to interact with the above APIs. OPERATIONAL EXPERIENCE ➤Knative serving helps Ops with releasing, auto-scaling, configuring services. Knative eventing with abstracting the broker.  ➤Dapr helps Ops with monitoring, securing, and increasing resiliency of services, as well as cloud infrastructure abstraction. TOP FEATURES ➤Knative Scale to 0 Autoscaling Traffic splitting App definition Pub/sub Connectors Function CLI ➤Dapr Pub/sub Service-to-service interaction with resiliency, key/value access, Actors, Connectors, Security (mTLS, Auth), Config/Secrets, Workflows, Distributed Lock APIs. FEATURE OVERLAP The primary overlap is around pub/sub capabilities. Both projects offer async interactions between applications by abstracting the broker &amp;amp; using CloudEvents format. Both projects have connectors and ability to subscribe apps to the broker.  ➤Knative defines these w/ CRDs &amp;amp; over HTTP.  ➤Dapr supports HTTP/gRPC, using CRDs and code. SWEET SPOT ➤Knative: autoscaling containers (to and from zero). ➤Dapr: event-driven and stateful service interactions. Upcoming hot feature:  ➤Knative: function development.  ➤Dapr: workflows based orchestration. SUMMARY Those are the key differences between Dapr and Knative I'm aware of.  If you know other differences, share those on the twitter . We are actively working on both projects and exploring how they complement each other and integrate with the greater CNCF .</content><dc:creator>Unknown</dc:creator></entry><entry><title>How JBoss EAP 8-Beta makes deployment on OpenShift easier</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/08/how-jboss-eap-8-beta-makes-deployment-openshift-easier" /><author><name>Philip Hayes</name></author><id>64d51ce9-7c63-4ea8-a224-fbd3fe72cd98</id><updated>2023-02-08T07:00:00Z</updated><published>2023-02-08T07:00:00Z</published><summary type="html">&lt;p&gt;The recent &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/8-beta/html/release_notes_for_red_hat_jboss_enterprise_application_platform_8.0_beta/index?extIdCarryOver=true&amp;sc_cid=701f2000001Css5AAC"&gt;release of Red Hat JBoss EAP 8-Beta&lt;/a&gt; introduced changes to the provisioning and configuration of JBoss EAP application images on &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The process adopted the JBoss &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/8-beta/html-single/using_jboss_eap_on_openshift_container_platform/index#assembly_provisioning-a-jboss-eap-server-using-the-maven-plugin_assembly_environment-variables-and-model-expression-resolution"&gt;EAP Maven Plugin&lt;/a&gt;, which provides significant improvements making the configuration of JBoss EAP on OpenShift easier and more flexible.&lt;/p&gt; &lt;p&gt;This article demonstrates the steps required to add the JBoss EAP Maven Plugin to an existing JBoss EAP 8-Beta application. We will also test our plugin locally and then build and deploy our application to OpenShift using Helm via the OpenShift UI and the Helm CLI.&lt;/p&gt; &lt;h2&gt;The benefits of the JBoss EAP Maven plugin&lt;/h2&gt; &lt;p&gt;The JBoss EAP Maven plugin provides the following functionalities:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Uses the wildfly-ee-galleon-pack and &lt;a href="https://github.com/jbossas/eap-cloud-galleon-pack/blob/main/doc/index.md"&gt;eap-cloud-galleon-pack&lt;/a&gt; Galleon feature-packs and selection of layers for customizing the server configuration file.&lt;/li&gt; &lt;li aria-level="1"&gt;Applies CLI script commands to the server. These CLI scripts can be easily added to the application source code repository.&lt;/li&gt; &lt;li aria-level="1"&gt;Supports the addition of extra files into the server installation, such as a keystore file.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Refer to the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/8-beta/html-single/using_jboss_eap_on_openshift_container_platform/index#assembly_provisioning-a-jboss-eap-server-using-the-maven-plugin_assembly_environment-variables-and-model-expression-resolution"&gt;documentation&lt;/a&gt; for more information. Refer to the &lt;a href="https://github.com/jboss-developer/jboss-eap-quickstarts/tree/8.0.x"&gt;quickstarts for JBoss EAP 8-Beta&lt;/a&gt; where you will find working examples for your project.&lt;/p&gt; &lt;p&gt;When building a JBoss EAP 7.x image for OpenShift, the OpenShift builder image provisioned the JBoss EAP server instance. The configuration was provided by a combination of runtime variables, configuration snippets, and JBoss CLI scripts.&lt;/p&gt; &lt;p&gt;With JBoss EAP 8-Beta, the JBoss EAP Maven plugin provisions the server and deploys the packaged application during the Maven execution. All the configuration for this build process is maintained in the Maven pom.xml file. This allows developers and operations teams to control and test their EAP deployments in their local environments, which provides significant benefits.&lt;/p&gt; &lt;p&gt;The JBoss EAP Maven plugin uses &lt;a href="https://github.com/wildfly/galleon#overview"&gt;Galleon&lt;/a&gt; to provision a JBoss EAP server configured with the minimum set of features to support the deployed application. Galleon is a provisioning tool for working with Maven repositories. Galleon automatically retrieves released JBoss EAP Maven artifacts to compose a software distribution of a JBoss EAP-based application server according to a user's configuration.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://helm.sh/docs/intro/install/"&gt;Helm CLI&lt;/a&gt;. Version 3.5+  (only required if you want to use Helm CLI instead of OpenShift Dev Console)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift"&gt;OpenShift cluster&lt;/a&gt;. Version 4.11+&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://maven.apache.org/download.cgi"&gt;Maven&lt;/a&gt;. Version 3.8.5+&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Project setup&lt;/h2&gt; &lt;p&gt;We're going to start with a simple "hello world" JBoss EAP 8-Beta application. You can check out the source from the &lt;a href="https://github.com/deewhyweb/eap8-on-ocp"&gt;GitHub page&lt;/a&gt;. We can create the deployment artifact from this project by running the following command from the folder you cloned your code into.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn clean package&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Our application artifact should now be available at:  &lt;strong&gt;./target/helloworld.war&lt;/strong&gt;. If you want to test this application in JBoss EAP 8-Beta, you can do so by following these instructions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Download the JBoss EAP 8-Beta distribution &lt;a href="https://developers.redhat.com/products/eap/download"&gt;here&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Extract the distribution into a local folder (e.g., ~/jboss-eap-8).&lt;/li&gt; &lt;li&gt;Set the EAP_HOME environment variable as follows:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;export EAP_HOME=~/jboss-eap-8&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Start JBoss EAP 8-Beta with the following command:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;$EAP_HOME/bin/standalone.sh&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Connect to EAP using the CLI from the folder you cloned your code into by running the following command:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;$EAP_HOME/bin/jboss-cli.sh --connect&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Deploy our application artifact as follows:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;[standalone@localhost:9990 /] deploy ./target/helloworld.war&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;You should now be able to access the sample application at:  &lt;a href="http://localhost:8080/helloworld/HelloWorld"&gt;http://localhost:8080/helloworld/HelloWorld&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Shut down the JBoss EAP 8-Beta server.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Adding OpenShift support&lt;/h2&gt; &lt;p&gt;To add OpenShift support, we're going to add the JBoss EAP Maven plugin to our project and create an OpenShift profile. To do this, add the following to the pom.xml file in the &lt;strong&gt;&lt;profiles&gt;&lt;/strong&gt; section:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;        &lt;profile&gt;             &lt;id&gt;openshift&lt;/id&gt;             &lt;build&gt;                 &lt;plugins&gt;                     &lt;plugin&gt;                         &lt;groupId&gt;org.jboss.eap.plugins&lt;/groupId&gt;                         &lt;artifactId&gt;eap-maven-plugin&lt;/artifactId&gt;                         &lt;version&gt;1.0.0.Beta6-redhat-00001&lt;/version&gt;                         &lt;configuration&gt;                             &lt;channels&gt;                                 &lt;channel&gt;                                     &lt;groupId&gt;org.jboss.eap.channels&lt;/groupId&gt;                                     &lt;artifactId&gt;eap-8.0-beta&lt;/artifactId&gt;                                 &lt;/channel&gt;                             &lt;/channels&gt;                             &lt;feature-packs&gt;                                 &lt;feature-pack&gt;                                     &lt;location&gt;org.jboss.eap:wildfly-ee-galleon-pack&lt;/location&gt;                                 &lt;/feature-pack&gt;                                 &lt;feature-pack&gt;                                     &lt;location&gt;org.jboss.eap.cloud:eap-cloud-galleon-pack&lt;/location&gt;                                 &lt;/feature-pack&gt;                             &lt;/feature-packs&gt;                             &lt;layers&gt;                                 &lt;layer&gt;cloud-server&lt;/layer&gt;                             &lt;/layers&gt;                             &lt;filename&gt;ROOT.war&lt;/filename&gt;                         &lt;/configuration&gt;                         &lt;executions&gt;                             &lt;execution&gt;                                 &lt;goals&gt;                                     &lt;goal&gt;package&lt;/goal&gt;                                 &lt;/goals&gt;                             &lt;/execution&gt;                         &lt;/executions&gt;                     &lt;/plugin&gt;                 &lt;/plugins&gt;             &lt;/build&gt;         &lt;/profile&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Introducing this profile to our maven configuration makes the following updates:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Creates an "openshift" profile with a "package" goal.&lt;/li&gt; &lt;li aria-level="1"&gt;Adds the eap-maven-plugin.&lt;/li&gt; &lt;li aria-level="1"&gt;Configures the eap-maven-plugin with feature packs. Feature packs are ZIP archives that are normally deployed to artifact repositories (such as Maven), where they can be used by Galleon tools. These features are then defined in predefined configuration layers that can be used to create installation configurations. The use of layers from feature packs enables the deployment of an EAP instance, providing minimal features to support the deployed application. For this project, we're adding two feature packs: &lt;ul&gt;&lt;li aria-level="2"&gt;org.jboss.eap:wildfly-ee-galleon-pack &lt;ul&gt;&lt;li aria-level="3"&gt;The wildfly-ee-galleon-pack contains the features required to build an instance of JBoss EAP. This feature pack contains several layers (e.g., jaxrs-server and cloud-server).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;org.jboss.eap.cloud:eap-cloud-galleon-pack &lt;ul&gt;&lt;li aria-level="3"&gt;The org.jboss.eap.cloud:eap-cloud-galleon-pack Galleon feature-pack provisions a set of additional features allowing you to configure a JBoss EAP server to run on the cloud.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Inclusion of the cloud-server layer. The cloud-server layer is the minimum layer required for applications deployed on OpenShift. This layer provides subsystems such as health and metrics.&lt;/li&gt; &lt;li aria-level="1"&gt;Setting the filename to ROOT.war ensures the application is deployed in the root context.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Testing the JBoss EAP Maven plugin&lt;/h2&gt; &lt;p&gt;Now that we've added the OpenShift profile and eap-maven-plugin, we can test the provisioning of a JBoss EAP 8-Beta instance. To do this, we run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn package -Popenshift&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will perform these tasks:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Create a &lt;strong&gt;/target/server&lt;/strong&gt; folder and deploy an instance of JBoss EAP. The JBoss EAP instance will only contain the functionality defined by the &lt;strong&gt;&lt;layers&gt;&lt;/strong&gt; section of the pom.xml file, in this case, the cloud-server layer.&lt;/li&gt; &lt;li aria-level="1"&gt;Package the project code and name the resulting artifact &lt;strong&gt;ROOT.war&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Start the JBoss EAP instance from the &lt;strong&gt;/target/server&lt;/strong&gt; folder and deploy the &lt;strong&gt;ROOT.war&lt;/strong&gt; artifact.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The output of the command should be as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[INFO] Deploying ROOT.war [disconnected /] embed-server --server-config=standalone.xml [standalone@embedded /] deploy  ROOT.war --name=ROOT.war --runtime-name=ROOT.war [standalone@embedded /] stop-embedded-server&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can test the instance of JBoss EAP provisioned by the JBoss EAP Maven plugin by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;./target/server/bin/standalone.sh&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will start JBoss EAP 8-Beta. Looking at the logs, you should see a couple of things of note:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;INFO [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0010: Deployed "ROOT.war" (runtime-name : "ROOT.war")&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The packaged application has been deployed as ROOT.war. This will ensure the application is deployed to the root context.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0054: Admin console is not enabled&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The admin console is not enabled for this instance of JBoss EAP. This is the recommended approach for JBoss EAP applications deployed on OpenShift.&lt;/p&gt; &lt;p&gt;Navigate to &lt;a href="http://localhost:8080"&gt;http://localhost:8080&lt;/a&gt; (Figure 1).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/local_1.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/local_1.jpg?itok=ALQoRl_h" width="600" height="417" alt="The Hello World application running on localhost." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The JBoss EAP Hello World application running on the localhost.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now that we are confident the eap-maven-plugin is configured correctly, we can move on to deploy the application to OpenShift.&lt;/p&gt; &lt;h2&gt;Deploying to OpenShift&lt;/h2&gt; &lt;p&gt;To deploy our application to OpenShift, we will utilize Helm charts using the OpenShift UI and the Helm CLI tool.&lt;/p&gt; &lt;p&gt;First, we need to create a Helm configuration by following these steps:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Create a folder called "charts" in the folder you cloned the eap8-on-ocp code into.&lt;/li&gt; &lt;li&gt;In this folder, create a file called "helm.yaml" with the following contents:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-yaml"&gt;build:   uri: https://github.com/deewhyweb/eap8-on-ocp.git   ref: 8.0.x deploy:   replicas: 1&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;In this example, we're using a pre-prepared branch containing source code updated to include the eap-maven-plugin configuration. You can also use your repository by replacing the information in this yaml file.&lt;/li&gt; &lt;li&gt;You will need to push the changes to pom.xml described above for the build to execute correctly.&lt;/li&gt; &lt;li&gt;We're now ready to deploy our application to OpenShift.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Deploying the Helm chart using the OpenShift UI&lt;/h3&gt; &lt;p&gt;Follow these steps to deploy the Helm chart in the OpenShift UI:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Log in to OpenShift.&lt;/li&gt; &lt;li&gt;Open the Developer UI. &lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;Helm&lt;/strong&gt; in the left-hand menu.&lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;Install a Helm Chart from the developer catalog&lt;/strong&gt;. This will bring you to the &lt;strong&gt;Helm Charts&lt;/strong&gt; catalog page (Figure 2).&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Filter by keyword&lt;/strong&gt; field, enter:  "eap"&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/helm-catalog.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/helm-catalog.jpg?itok=ojh4PkxQ" width="600" height="236" alt="The Helm charts catalog in OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The Helm charts catalog in OpenShift.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li&gt;From the list of Helm charts, select &lt;strong&gt;eap8&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;Install Helm Chart&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Switch to the &lt;strong&gt;YAML view&lt;/strong&gt; &lt;/li&gt; &lt;li&gt;Paste the configuration we created in charts/helm.yaml, as shown in Figure 3.&lt;/li&gt; &lt;li&gt;Click on &lt;strong&gt;Install&lt;/strong&gt; to deploy the Helm chart.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/install-helm-chart.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/install-helm-chart.jpg?itok=-DPRlPPe" width="600" height="408" alt="The screen to install EAP Helm chart." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The screen to install the EAP Helm chart on OpenShift from the Helm catalog.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt; &lt;/p&gt; &lt;h3&gt;Deploying the Helm chart using the Helm CLI&lt;/h3&gt; &lt;p&gt;Now we need to deploy the Helm chart. We will use &lt;span&gt; the Helm CLI. Follow these steps:&lt;/span&gt;&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;span&gt;Use the jboss-eap/eap8 Helm chart with our configuration:&lt;/span&gt;&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;helm install helloworld -f charts/helm.yaml --repo https://jbossas.github.io/eap-charts/ eap8&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;When this command completes, you should see something like this:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;NAME: helloworld LAST DEPLOYED: Fri Dec 16 12:52:27 2022 NAMESPACE: eap8-helm STATUS: deployed REVISION: 1 TEST SUITE: None NOTES:&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Your EAP 8 application is building! To follow the build, run this command:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;oc get build -w&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(Note: Your deployment will report &lt;strong&gt;ErrImagePull&lt;/strong&gt; and &lt;strong&gt;ImagePullBackOff&lt;/strong&gt; until the build is complete. Once the build is complete, your image will be automatically rolled out.)&lt;/p&gt; &lt;ul&gt;&lt;li&gt;To follow the deployment of your application, run the following command:&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-bash"&gt;oc get deployment helloworld -w&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Testing the application deployment&lt;/h3&gt; &lt;p&gt;In the OpenShift UI, you should see two builds run and complete, then a pod will be deployed containing your JBoss EAP instance and application deployed (Figure 4).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_0_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_0_0.jpg?itok=7YpNZ6Pf" width="600" height="375" alt="JBoss EAP 8-Beta application deployed on OpenShift." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The JBoss EAP 8-Beta application deployed on OpenShift.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;To test your application, click on the route icon. Alternatively, to find the route with the OpenShift CLI, enter this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get routes&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see something this output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;NAME         HOST/PORT                                                             PATH   SERVICES     PORT    TERMINATION     WILDCARD helloworld   helloworld-eap8-helm.apps.cluster-xxx.xxx.hostname.com          helloworld   &lt;all&gt;   edge/Redirect   None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Navigate to this route with http:// + host in your browser.&lt;/p&gt; &lt;p&gt;You should see &lt;strong&gt;Hello World!&lt;/strong&gt; as with the local host instance we deployed earlier. Our application has now been successfully built and deployed to OpenShift.&lt;/p&gt; &lt;h2&gt;Recap&lt;/h2&gt; &lt;p&gt;JBoss EAP 8-Beta uses the eap-maven-plugin to provision and configure EAP for OpenShift images. In this article, we demonstrated how to add the eap-maven-plugin to an existing JBoss EAP 8-Beta application and the associated benefits. We then use this plugin to test the provisioning of a JBoss EAP 8-Beta server locally, and then deploy this application to OpenShift using the EAP8 Helm chart.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/08/how-jboss-eap-8-beta-makes-deployment-openshift-easier" title="How JBoss EAP 8-Beta makes deployment on OpenShift easier"&gt;How JBoss EAP 8-Beta makes deployment on OpenShift easier&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Philip Hayes</dc:creator><dc:date>2023-02-08T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 26 January 2023</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2023-02-08.html" /><category term="serverless" /><category term="dashbuilder" /><category term="keycloak" /><category term="vertx" /><category term="wildfly" /><category term="openshift" /><author><name>Pedro Silva</name><uri>https://www.jboss.org/people/pedro-silva</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2023-02-08.html</id><updated>2023-02-08T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="serverless, dashbuilder, keycloak, vertx, wildfly, openshift"&gt; &lt;h1&gt;This Week in JBoss - 26 January 2023&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;It’s great to be back and bringing you the 2nd edition of the JBoss Editorial of 2023. We have a lot of exciting news and updates from JBoss world, so enjoy.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases_releases_releases"&gt;Releases, releases, releases…​&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-3-8/"&gt;Eclipse Vert.x 4.3.8 released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-16-2-final-released/"&gt;Quarkus 2.16.2.Final released - Maintenance release&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_serverless_workflow_validations"&gt;Serverless workflow validations&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/01/serverless-workflow-validations.html/"&gt;Serverless workflow validations&lt;/a&gt; by Saravana Balaji&lt;/p&gt; &lt;p&gt;Writing a Serverless Workflow that matches with the specification’s rules and schema can require some documentation reading, which demands a few hours. In this article Saravana show how to implement a validation mechanism on Serverless Workflow Editor, that checks JSON and YAML files against Serverless Workflow specifications schema and also provides some custom validations in addition to it.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_new_feature_dashbuilder_editor_with_intellisense_capabilities"&gt;New Feature: Dashbuilder Editor With Intellisense Capabilities&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/02/new-feature-dashbuilder-editor-with-intellisense-capabilities.html"&gt;New Feature: Dashbuilder Editor With Intellisense Capabilities&lt;/a&gt; by Ajay Jaganathan&lt;/p&gt; &lt;p&gt;In this great blog post Ajay introduces the new dashboard feature that now have auto-complete capabilities. This provides the user experience by providing suggestions and helps to reduce the errors made while authoring the dashbuilder specification.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_synchronize_your_kie_sandbox_workspace_with_bitbucket_or_github"&gt;Synchronize Your Kie Sandbox Workspace With Bitbucket Or Github&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2023/02/synchronize-your-kie-sandbox-workspace-with-bitbucket-or-github.html"&gt;Synchronize Your Kie Sandbox Workspace With Bitbucket Or Github&lt;/a&gt; by Jan Stastny&lt;/p&gt; &lt;p&gt;Here is the news, KIE Sandbox now brings the possibility to synchronize your changes not only with GitHub, but also Bitbucket.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_how_jboss_eap_8_beta_makes_deployment_on_openshift_easier"&gt;How JBoss EAP 8-Beta makes deployment on OpenShift easier&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2023/02/08/how-jboss-eap-8-beta-makes-deployment-openshift-easier"&gt;How JBoss EAP 8-Beta makes deployment on OpenShift easier&lt;/a&gt; by Philip Hayes&lt;/p&gt; &lt;p&gt;The new version of Red Hat JBoss EAP 8-Beta introduced changes to the provisioning and configuration of JBoss EAP application images on Red Hat OpenShift. In this great article, Philip shows the easiest way to put the EAP 8-Beta on the cloud using Openshift.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_how_to_use_a_datasource_in_quarkus"&gt;How to use a Datasource in Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/soa-cloud/quarkus/how-to-use-a-datasource-in-quarkus/"&gt;How to use a Datasource in Quarkus&lt;/a&gt; by F.Marchioni&lt;/p&gt; &lt;p&gt;If you need to know how to use a Datasource in Quarkus using Agroal, here is a great article to learn about it. Enjoy the reading.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_keycloak_tutorial_for_beginners"&gt;Keycloak tutorial for beginners&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/keycloak/introduction-to-keycloak/"&gt;Keycloak tutorial for beginners&lt;/a&gt; by F.Marchioni&lt;/p&gt; &lt;p&gt;In this tutorial, Marchioni show the first steps on the Keycloak world. You will know how to start and configure the Keycloak based on Wildfly and also, the Keycloak based on Quarkus.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_how_to_compress_logs_in_wildfly"&gt;How to compress logs in WildFly&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-compress-logs-in-wildfly/"&gt;How to compress logs in WildFly&lt;/a&gt; by F.Marchioni&lt;/p&gt; &lt;p&gt;This article shows how to enable logs compression in WildFly by setting the appropriate suffix in your Periodic Rotating File Handler. In the second part of this tutorial we will learn how to compress logs using Log4j instead.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_how_to_print_logs_in_json_format_in_wildfly"&gt;How to print logs in JSON format in WildFly&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-print-logs-in-json-format-in-wildfly/"&gt;How to print logs in JSON format in WildFly&lt;/a&gt; by F.Marchioni&lt;/p&gt; &lt;p&gt;If you need to use JSON format on your Wildfly logs, you would like to read this another great Marchioni tutorial.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_datacater_uses_quarkus_to_make_data_streaming_more_accessible"&gt;DataCater uses Quarkus to make Data Streaming more accessible&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/datacater-uses-quarkus-to-make-data-streaming-accessible/"&gt;DataCater uses Quarkus to make Data Streaming more accessible&lt;/a&gt; by Stefan Sprenger&lt;/p&gt; &lt;p&gt;This article gives a brief overview of the data streaming platform DataCater, discusses how we moved from Scala Play! and Kafka Streams to Quarkus, and presents why we think that Quarkus is an exceptional framework for developing cloud-native Java applications.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_youtube_videos"&gt;YouTube videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Definitely catch the replay of &lt;a href="https://www.youtube.com/watch?v=95GsShVW6rY&amp;#38;list=PLsM3ZE5tGAVatO65JIxgskQh-OKoqM4F2"&gt;Quarkus Insights #116: Continuous delivery with Quarkus Helm and ArgoCD&lt;/a&gt; if you haven’t already watched it.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_decaf"&gt;Decaf'&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;If you want to learn more about AI on Openshit, you need to know the &lt;a href="https://ai-on-openshift.io/"&gt;AI ON Opensfhit&lt;/a&gt; with demos, patterns, and other Machine Learn, MLOps, and AI stuff. You also may want to more about MLOps and Open Data Hub project. Open Data Hub (ODH) is an open source project that provides open source AI tools for running large and distributed AI workloads on the OpenShift Container Platform. So, go ahead and read the &lt;a href="https://opendatahub.io/news/2023-02-06/docs-datascienceprojects.html"&gt;new Open Data Hub documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;That’s all folks! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/pedro-silva.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Pedro Silva&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Pedro Silva</dc:creator></entry><entry><title type="html">How to print logs in JSON format in WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-print-logs-in-json-format-in-wildfly/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/how-to-print-logs-in-json-format-in-wildfly/</id><updated>2023-02-06T09:32:44Z</updated><content type="html">This article discusses the configuration you can apply in WildFly to enable logging in JSON format. We will learn how to do that natively in WildFly or using a Log4j custom configuration. Strategies for logging in JSON There are several use cases in which logging in JSON format can be useful. For example: Use cases ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to compress logs in WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-compress-logs-in-wildfly/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/how-to-compress-logs-in-wildfly/</id><updated>2023-02-06T09:12:23Z</updated><content type="html">This article shows how to enable logs compression in WildFly by setting the appropriate suffix in your Periodic Rotating File Handler. In the second part of this tutorial we will learn how to compress logs using Log4j instead. Compressing Logs natively with WildFly Logs are an essential part of any software application, as they help ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to set the SameSite attribute in Java Web applications</title><link rel="alternate" href="http://www.mastertheboss.com/web/jboss-web-server/how-to-set-the-samesite-attribute-in-java-web-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/web/jboss-web-server/how-to-set-the-samesite-attribute-in-java-web-applications/</id><updated>2023-02-06T07:43:46Z</updated><content type="html">This short article describes how you can set the SameSite property in HTTP Cookies for Web applications, with special focus on WildFly‘s Web server, which is Undertow. What is SameSite ? SameSite is a property that you can set in HTTP cookies to avoid false cross-site request (CSRF) attacks in web applications: When SameSite is ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to improve application security using _FORTIFY_SOURCE=3</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/02/06/how-improve-application-security-using-fortifysource3" /><author><name>Siddhesh Poyarekar</name></author><id>4972acb7-3c57-4a83-94dd-f13813593ac1</id><updated>2023-02-06T07:00:00Z</updated><published>2023-02-06T07:00:00Z</published><summary type="html">&lt;p&gt;Last year I wrote about the &lt;a href="https://developers.redhat.com/articles/2022/09/17/gccs-new-fortification-level"&gt;new level for _FORTIFY_SOURCE&lt;/a&gt; and how it promises to significantly improve application security mitigation in C/C++. In this article, I will show you how an application or library developer can get the best possible fortification results from the compiler to improve the security of applications deployed on &lt;a href="https://www.redhat.com/en/technologies/linux-platforms/enterprise-linux"&gt;Red Hat Enterprise Linux&lt;/a&gt;, for instance. There are shades of previous articles about GCC. But that just goes to show how compiler features tie in together to provide security protection at multiple levels, from prevention to mitigation. First, we should take a closer look at the potential impact of &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; on performance and code size of applications.&lt;/p&gt; &lt;h2&gt;The performance impact of the new fortification level&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; builtin improves fortification coverage by evaluating and passing size expressions instead of the constants seen in &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;, which generates additional code and potentially more register pressure. But the impact of that additional code appears to be trivial in practice. When I compared nearly &lt;a href="https://docs.google.com/spreadsheets/d/1nPSmbEf3HVB91zI8yBraMqVry3_ILmlV2Z5K7FZeHZg/edit?usp=sharing"&gt;10 thousand packages&lt;/a&gt; in Fedora rawhide, I found barely any impact on code size. Some binaries grew while others shrunk, indicating a change in generated code, but there was no broad increase in code size.&lt;/p&gt; &lt;p&gt;However, given that the code did change, surely we should see side effects such as register pressure, shouldn't we? Again in practice, that side effect turns out to be trivial. Running SPEC benchmarks with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; again showed no slowdown at all compared to &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;, indicating that there is no broad-based impact on performance due to this new fortification level. The results are not entirely surprising, though, if you put them in the context of typical programs, modern processors, and how &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; works.&lt;/p&gt; &lt;h3&gt;Does object size overhead affect performance?&lt;/h3&gt; &lt;p&gt;At a high level, the major purpose of the &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; feature is to estimate the size of an object passed to a library function call and ensure that the call does not perform any unsafe actions on that object and abort if it does. The success of &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; as a mitigation strategy is directly linked to its ability to estimate the size of the passed object.&lt;/p&gt; &lt;p&gt;Now there are two main vectors for performance overhead due to this:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Overhead of a fortified call instead of the regular call (e.g., &lt;code&gt;__memcpy_chk&lt;/code&gt; for &lt;code&gt;memcpy&lt;/code&gt;). This is significant because, in theory, &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; should generate many more of these than &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Overhead of the size expression that is passed to the fortified call instead of the constant in &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The function call overhead isn't a big enough concern for two main reasons. The most important reason is that in many cases where the size of an object is visible, the compiler is determined conclusively at compile time that the access is safe. Thank the wonderful work on &lt;a href="https://developers.redhat.com/blog/2021/04/28/value-range-propagation-in-gcc-with-project-ranger"&gt;value range propagation&lt;/a&gt; that went into GCC in recent years for this. Due to this, the compiler can, in those cases, avoid fortifying the call and instead use the regular library function call.&lt;/p&gt; &lt;p&gt;In cases where the fortified call is unavoidable, the overhead will be noticeable only if the call is encountered repeatedly (i.e., it is on the hot path). Here's where modern CPUs come into the picture with their well-oiled branch predictors. The branches for access safety validation are always predicted correctly, and the processor sails through them almost as if they weren't there.&lt;/p&gt; &lt;p&gt;The overhead of size expressions is slightly trickier to explain but still intuitive enough. Whenever &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; is successful in determining the size estimate for an object, it basically has access to the definition of that object, which either gives it a readily available constant or expression for use. Additionally, any derivative arithmetic the compiler needs to generate for the object access (e.g., &lt;code&gt;&amp;buf-&gt;member.data[1] + i&lt;/code&gt;) is often the same arithmetic to get the final size of the pointer, which the compiler appears to reliably meld together, thereby nullifying any such overhead.&lt;/p&gt; &lt;h3&gt;Final verdict on performance impact&lt;/h3&gt; &lt;p&gt;One might be tempted to conclude that there is absolutely no performance overhead to building applications with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; , but it is more nuanced than that. In most cases, the performance overhead appears negligible due to the compiler being smart enough to optimize most of the overhead away. As a result, it should be safe for most application developers to simply bump up fortification to &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; and be done with it.&lt;/p&gt; &lt;p&gt;Now let's look at how application developers can get the most out of &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;How to improve application fortification&lt;/h2&gt; &lt;p&gt;The primary way to improve the success of &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; is to tell the compiler about the size of an object passed into a function. The compiler can evaluate simple cases where objects are plain types or structures with constant sizes almost all the time. However, objects that are dynamically allocated and whose pointers are passed to a function are tricky. There are several ways to tell the compiler that. These hints are supported by GCC and Clang, so it does not matter which of those two compilers you use. Additionally, these attributes can be applied to C and C++ functions, so this is not limited to just C.&lt;/p&gt; &lt;p&gt;Note that these benefits don't just improve fortification. Since they end up giving better object size information, they improve overall diagnostics, which means better warnings and often even faster code.&lt;/p&gt; &lt;h3&gt;Using allocator functions&lt;/h3&gt; &lt;p&gt;If your application uses allocators provided by the standard library (e.g., &lt;code&gt;malloc&lt;/code&gt;, &lt;code&gt;realloc&lt;/code&gt;, etc.), the compiler can automatically use the size argument passed to those functions as the object size. However, if your application has wrappers that do special things before or after allocation, or if your application has bespoke allocator functions, you could decorate those functions with the &lt;code&gt;__alloc_size__&lt;/code&gt; attribute to indicate which of the arguments to your allocation function is the size of the returned object.&lt;/p&gt; &lt;p&gt;This is how it would look:&lt;/p&gt; &lt;p&gt;&lt;code&gt;void *my_allocator (size_t sz) __attribute__ ((__alloc_size__ (1)));&lt;/code&gt;&lt;/p&gt; &lt;p&gt;For a calloc-like allocator, it would be:&lt;/p&gt; &lt;p&gt;&lt;code&gt;void *my_allocator (size_t nmemb, size_t size) __attribute__ ((__alloc_size__ (1, 2)));&lt;/code&gt;&lt;/p&gt; &lt;p&gt;In the first case, the compiler will see that the size of the allocated object is &lt;code&gt;sz&lt;/code&gt;. In the second case, it will see the size as &lt;code&gt;nmemb * size&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;How to use the __access__ function attribute&lt;/h3&gt; &lt;p&gt;In C, a typical programming practice is that when pointers are passed to a function to access arrays, the size of the array the pointer points to is typically passed as another argument to that function. If your application or library uses this pattern, then you may be able to tell the compiler about this size using the &lt;code&gt;__access__&lt;/code&gt; function attribute on the definition of that function. This attribute is a GCC extension, also available in Clang. The following example tells the compiler that &lt;code&gt;ptr&lt;/code&gt; points to memory that is safe to read and write to the extent of &lt;code&gt;sz&lt;/code&gt; bytes.&lt;/p&gt; &lt;p&gt;&lt;code&gt;void&lt;br /&gt; __attribute__ ((__access__ (__read_write__, 1, 2)))&lt;br /&gt; do_something (char *ptr, size_t sz)&lt;/code&gt;&lt;br /&gt;&lt;code&gt;{&lt;br /&gt;   ...&lt;/code&gt;&lt;br /&gt;&lt;code&gt;  // Get a copy size from somewhere else.&lt;/code&gt;&lt;br /&gt;&lt;code&gt;  size_t setsize = get_size ();&lt;br /&gt;   memset (ptr, 0, setsize);&lt;br /&gt; }&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Put the attribute in the function declaration and the definition because the compiler uses it to validate call sites and perform analysis and fortification within the implementation. At the call site, the value passed for &lt;code&gt;sz&lt;/code&gt; is validated against the size of the object pointed to by &lt;code&gt;ptr&lt;/code&gt; to ensure that &lt;code&gt;do_something&lt;/code&gt; can safely access &lt;code&gt;sz&lt;/code&gt; elements in &lt;code&gt;ptr&lt;/code&gt;. Any inconsistency is flagged as a compile time warning. Inside the function implementation, &lt;code&gt;sz&lt;/code&gt; is assumed to be the size of &lt;code&gt;ptr&lt;/code&gt; and any accesses through &lt;code&gt;ptr&lt;/code&gt; within the function are validated against &lt;code&gt;sz&lt;/code&gt;. In the &lt;code&gt;do_something&lt;/code&gt; implementation, &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt; will ensure in the call to &lt;code&gt;memset&lt;/code&gt; that &lt;code&gt;setsize&lt;/code&gt; is less than or equal to &lt;code&gt;sz&lt;/code&gt; or otherwise, abort.&lt;/p&gt; &lt;p&gt;An important note about a known issue in the compiler attributes (i.e., &lt;code&gt;__alloc_size__&lt;/code&gt; and &lt;code&gt;__access__&lt;/code&gt;:) is that these are read by the compiler only if the function it is associated with is not inlined. That is, if &lt;code&gt;do_something&lt;/code&gt; or &lt;code&gt;my_allocator&lt;/code&gt; are inlined, the compiler won't see their attributes anymore. In common cases, this should not matter too much because the inlining ideally should give just as much or more information about the object size. My advice is to correctly annotate all of the functions in the application or library.&lt;/p&gt; &lt;h3&gt;The flexible array conundrum&lt;/h3&gt; &lt;p&gt;Flexible arrays are a complex topic because of the various ways GCC and Clang support them. A flexible array is an array at the end of a structure that is dynamically allocated in the program. Before it was standardized, GCC had an extension where any array that was declared at the end of the structure with subscripts &lt;code&gt;[0]&lt;/code&gt; and &lt;code&gt;[1]&lt;/code&gt; were considered flexible arrays. C99 then formalized this with the &lt;code&gt;[]&lt;/code&gt; notation without any numeric subscript and further locked down semantics, ensuring that the flexible array always appeared at the end of a top-level structure.&lt;/p&gt; &lt;p&gt;GCC, however, continues to support the extensions and even supports flexible arrays in nested structures and unions. This makes object size computations tricky because the compiler may sometimes see the flexible arrays as zero or one-sized arrays, causing spurious crashes with &lt;code&gt;_FORTIFY_SOURCE&lt;/code&gt;. These problems can be avoided if the application uses the standard &lt;code&gt;[]&lt;/code&gt; notation for its flexible arrays.&lt;/p&gt; &lt;h2&gt;Build your applications with _FORTIFY_SOURCE=3&lt;/h2&gt; &lt;p&gt;This article has described the implications of building your application or library with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; compared to &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt;. The improved fortification coverage helps to make your programs significantly safer than the current state. I have provided a &lt;a href="https://github.com/siddhesh/fortify-metrics"&gt;GCC plugin&lt;/a&gt; to help you measure the fortification coverage using &lt;code&gt;_FORTIFY_SOURCE=2&lt;/code&gt; compared to &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; so you can determine how much additional benefit it provides.&lt;/p&gt; &lt;p&gt;We hope to get closer to our goal of having safer applications deployed on &lt;a href="https://www.redhat.com/en/blog/hot-presses-red-hat-enterprise-linux-9"&gt;RHEL&lt;/a&gt;. We can accomplish this goal with more applications and libraries containing good compiler annotations and built with &lt;code&gt;_FORTIFY_SOURCE=3&lt;/code&gt; and with more developers fixing compiler warnings. If you have questions, please comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/02/06/how-improve-application-security-using-fortifysource3" title="How to improve application security using _FORTIFY_SOURCE=3"&gt;How to improve application security using _FORTIFY_SOURCE=3&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Siddhesh Poyarekar</dc:creator><dc:date>2023-02-06T07:00:00Z</dc:date></entry></feed>
